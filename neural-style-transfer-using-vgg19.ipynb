{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Nueral Style transfer with pre-trained VGG19 model**\n\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:42:57.321551Z","iopub.execute_input":"2022-06-18T16:42:57.321931Z","iopub.status.idle":"2022-06-18T16:42:57.327947Z","shell.execute_reply.started":"2022-06-18T16:42:57.321867Z","shell.execute_reply":"2022-06-18T16:42:57.327090Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Importing necessary libraries**","metadata":{}},{"cell_type":"code","source":"\nimport tensorflow as tf\nimport keras.preprocessing.image as process_im\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom keras.applications import vgg19\nfrom keras.models import Model\nfrom tensorflow.python.keras import models \nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import backend as K\nimport functools\nimport IPython.display","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-06-18T16:43:00.415979Z","iopub.execute_input":"2022-06-18T16:43:00.416523Z","iopub.status.idle":"2022-06-18T16:43:05.263754Z","shell.execute_reply.started":"2022-06-18T16:43:00.416457Z","shell.execute_reply":"2022-06-18T16:43:05.262824Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"content_path='../input/monalisa-picasso-dataset/Mona Lisa.jpg'\nstyle_path = '../input/monalisa-picasso-dataset/Picasso.jpg'","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:45:50.704430Z","iopub.execute_input":"2022-06-18T16:45:50.704778Z","iopub.status.idle":"2022-06-18T16:45:50.708793Z","shell.execute_reply.started":"2022-06-18T16:45:50.704727Z","shell.execute_reply":"2022-06-18T16:45:50.707947Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Define function to load images and return numpy array**","metadata":{}},{"cell_type":"code","source":"def load_file(image_path):\n    image =  Image.open(image_path)\n    max_dim=512\n    factor=max_dim/max(image.size)\n    image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n    im_array = process_im.img_to_array(image)\n    im_array = np.expand_dims(im_array,axis=0) #adding extra axis to the array as to generate a \n                                               #batch of single image \n    \n    return im_array","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:45:55.905936Z","iopub.execute_input":"2022-06-18T16:45:55.906315Z","iopub.status.idle":"2022-06-18T16:45:55.912898Z","shell.execute_reply.started":"2022-06-18T16:45:55.906259Z","shell.execute_reply":"2022-06-18T16:45:55.911915Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Define function to plot image**","metadata":{}},{"cell_type":"code","source":"def show_im(img,title=None):\n    img=np.squeeze(img,axis=0) #squeeze array to drop batch axis\n    plt.imshow(np.uint8(img))\n    if title is None:\n        pass\n    else:\n        plt.title(title)\n    plt.imshow(np.uint8(img))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:45:57.377953Z","iopub.execute_input":"2022-06-18T16:45:57.378405Z","iopub.status.idle":"2022-06-18T16:45:57.385410Z","shell.execute_reply.started":"2022-06-18T16:45:57.378345Z","shell.execute_reply":"2022-06-18T16:45:57.384474Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Plot Image**","metadata":{}},{"cell_type":"code","source":"content = load_file(content_path)\nstyle = load_file(style_path)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:45:59.639595Z","iopub.execute_input":"2022-06-18T16:45:59.639942Z","iopub.status.idle":"2022-06-18T16:45:59.985681Z","shell.execute_reply.started":"2022-06-18T16:45:59.639891Z","shell.execute_reply":"2022-06-18T16:45:59.984880Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\ncontent = load_file(content_path)\nstyle = load_file(style_path)\nplt.subplot(1,2,1)\nshow_im(content,'Content Image')\nplt.subplot(1,2,2)\nshow_im(style,'Style Image')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:46:02.271853Z","iopub.execute_input":"2022-06-18T16:46:02.272221Z","iopub.status.idle":"2022-06-18T16:46:03.037498Z","shell.execute_reply.started":"2022-06-18T16:46:02.272165Z","shell.execute_reply":"2022-06-18T16:46:03.036608Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Define function to process image for input to vgg19 model**","metadata":{}},{"cell_type":"code","source":"def img_preprocess(img_path):\n    image=load_file(img_path)\n    img=tf.keras.applications.vgg19.preprocess_input(image)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:46:08.272259Z","iopub.execute_input":"2022-06-18T16:46:08.272617Z","iopub.status.idle":"2022-06-18T16:46:08.279003Z","shell.execute_reply.started":"2022-06-18T16:46:08.272562Z","shell.execute_reply":"2022-06-18T16:46:08.277872Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Define function to deprocess image **","metadata":{}},{"cell_type":"markdown","source":"VGG networks are trained on image with each channel normalized by mean = [103.939, 116.779, 123.68]and with channels BGR.","metadata":{}},{"cell_type":"code","source":"def deprocess_img(processed_img):\n  x = processed_img.copy()\n  if len(x.shape) == 4:\n    x = np.squeeze(x, 0)\n  assert len(x.shape) == 3 #Input dimension must be [1, height, width, channel] or [height, width, channel]\n  \n  \n  # perform the inverse of the preprocessing step\n  x[:, :, 0] += 103.939\n  x[:, :, 1] += 116.779\n  x[:, :, 2] += 123.68\n  x = x[:, :, ::-1] # converting BGR to RGB channel\n\n  x = np.clip(x, 0, 255).astype('uint8')\n  return x","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:46:56.331232Z","iopub.execute_input":"2022-06-18T16:46:56.331735Z","iopub.status.idle":"2022-06-18T16:46:56.343479Z","shell.execute_reply.started":"2022-06-18T16:46:56.331670Z","shell.execute_reply":"2022-06-18T16:46:56.342031Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"im=img_preprocess(content_path)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:46:57.256862Z","iopub.execute_input":"2022-06-18T16:46:57.257329Z","iopub.status.idle":"2022-06-18T16:46:57.423730Z","shell.execute_reply.started":"2022-06-18T16:46:57.257270Z","shell.execute_reply":"2022-06-18T16:46:57.422830Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Get necessary layers from vgg19 model**","metadata":{}},{"cell_type":"code","source":"content_layers = ['block5_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\nnumber_content=len(content_layers)\nnumber_style =len(style_layers)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:46:59.368935Z","iopub.execute_input":"2022-06-18T16:46:59.369315Z","iopub.status.idle":"2022-06-18T16:46:59.374358Z","shell.execute_reply.started":"2022-06-18T16:46:59.369259Z","shell.execute_reply":"2022-06-18T16:46:59.373256Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Define function to get vgg19 model with pretrained weights**","metadata":{}},{"cell_type":"code","source":"def get_model():\n    \n    vgg=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n    vgg.trainable=False\n    content_output=[vgg.get_layer(layer).output for layer in content_layers]\n    style_output=[vgg.get_layer(layer).output for layer in style_layers]\n    model_output= style_output+content_output\n    return models.Model(vgg.input,model_output)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:00.096003Z","iopub.execute_input":"2022-06-18T16:47:00.096386Z","iopub.status.idle":"2022-06-18T16:47:00.102889Z","shell.execute_reply.started":"2022-06-18T16:47:00.096331Z","shell.execute_reply":"2022-06-18T16:47:00.101729Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\nmodel.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:47:01.181424Z","iopub.execute_input":"2022-06-18T16:47:01.181786Z","iopub.status.idle":"2022-06-18T16:47:13.329565Z","shell.execute_reply.started":"2022-06-18T16:47:01.181734Z","shell.execute_reply":"2022-06-18T16:47:13.327896Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Model architecture**","metadata":{}},{"cell_type":"code","source":"model=get_model()\nmodel.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:47:13.331380Z","iopub.execute_input":"2022-06-18T16:47:13.331729Z","iopub.status.idle":"2022-06-18T16:47:13.716514Z","shell.execute_reply.started":"2022-06-18T16:47:13.331678Z","shell.execute_reply":"2022-06-18T16:47:13.715793Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **Loss Functions**\n","metadata":{}},{"cell_type":"markdown","source":"**Define content loss**\n\nEssentially content loss captures the root mean squared error between the activations produced by the generated image and the content image.","metadata":{}},{"cell_type":"code","source":"def get_content_loss(noise,target):\n    loss = tf.reduce_mean(tf.square(noise-target))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:13.720290Z","iopub.execute_input":"2022-06-18T16:47:13.720560Z","iopub.status.idle":"2022-06-18T16:47:13.726064Z","shell.execute_reply.started":"2022-06-18T16:47:13.720515Z","shell.execute_reply":"2022-06-18T16:47:13.725114Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Define style loss**\n\n\nThe goal is to compute a style matrix for the generated image and the style image. Then the style loss is defined as the root mean square difference between the two style matrices. Style information is measured as the amount of correlation present between features maps in a given layer. Next, a loss is defined as the difference of correlation present between the feature maps computed by the generated image and the style image. The gram matrix is used to find the correlation between the feature maps of a convolution layer.","metadata":{}},{"cell_type":"code","source":"def gram_matrix(tensor):\n    channels=int(tensor.shape[-1])\n    vector=tf.reshape(tensor,[-1,channels])\n    n=tf.shape(vector)[0]\n    gram_matrix=tf.matmul(vector,vector,transpose_a=True)\n    return gram_matrix/tf.cast(n,tf.float32)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:13.728157Z","iopub.execute_input":"2022-06-18T16:47:13.728548Z","iopub.status.idle":"2022-06-18T16:47:13.735743Z","shell.execute_reply.started":"2022-06-18T16:47:13.728496Z","shell.execute_reply":"2022-06-18T16:47:13.734883Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def get_style_loss(noise,target):\n    gram_noise=gram_matrix(noise)\n    #gram_target=gram_matrix(target)\n    loss=tf.reduce_mean(tf.square(target-gram_noise))\n    return loss\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:13.737932Z","iopub.execute_input":"2022-06-18T16:47:13.738222Z","iopub.status.idle":"2022-06-18T16:47:13.747079Z","shell.execute_reply.started":"2022-06-18T16:47:13.738163Z","shell.execute_reply":"2022-06-18T16:47:13.746192Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_features(model,content_path,style_path):\n    content_img=img_preprocess(content_path)\n    style_image=img_preprocess(style_path)\n    \n    content_output=model(content_img)\n    style_output=model(style_image)\n    \n    content_feature = [layer[0] for layer in content_output[number_style:]]\n    style_feature = [layer[0] for layer in style_output[:number_style]]\n    return content_feature,style_feature\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:13.750272Z","iopub.execute_input":"2022-06-18T16:47:13.750590Z","iopub.status.idle":"2022-06-18T16:47:13.757954Z","shell.execute_reply.started":"2022-06-18T16:47:13.750538Z","shell.execute_reply":"2022-06-18T16:47:13.756882Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Define function to compute total loss**","metadata":{}},{"cell_type":"code","source":"def compute_loss(model, loss_weights,image, gram_style_features, content_features):\n    style_weight,content_weight = loss_weights #style weight and content weight are user given parameters\n                                               #that define what percentage of content and/or style will be preserved in the generated image\n    \n    output=model(image)\n    content_loss=0\n    style_loss=0\n    \n    noise_style_features = output[:number_style]\n    noise_content_feature = output[number_style:]\n    \n    weight_per_layer = 1.0/float(number_style)\n    for a,b in zip(gram_style_features,noise_style_features):\n        style_loss+=weight_per_layer*get_style_loss(b[0],a)\n        \n    \n    weight_per_layer =1.0/ float(number_content)\n    for a,b in zip(noise_content_feature,content_features):\n        content_loss+=weight_per_layer*get_content_loss(a[0],b)\n        \n    style_loss *= style_weight\n    content_loss *= content_weight\n    \n    total_loss = content_loss + style_loss\n    \n    \n    return total_loss,style_loss,content_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:14.172799Z","iopub.execute_input":"2022-06-18T16:47:14.173293Z","iopub.status.idle":"2022-06-18T16:47:14.183311Z","shell.execute_reply.started":"2022-06-18T16:47:14.173093Z","shell.execute_reply":"2022-06-18T16:47:14.182147Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Define function to calculate gradient**","metadata":{}},{"cell_type":"code","source":"def compute_grads(dictionary):\n    with tf.GradientTape() as tape:\n        all_loss=compute_loss(**dictionary)\n        \n    total_loss=all_loss[0]\n    return tape.gradient(total_loss,dictionary['image']),all_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:15.068564Z","iopub.execute_input":"2022-06-18T16:47:15.068911Z","iopub.status.idle":"2022-06-18T16:47:15.074395Z","shell.execute_reply.started":"2022-06-18T16:47:15.068860Z","shell.execute_reply":"2022-06-18T16:47:15.073243Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:47:16.113170Z","iopub.execute_input":"2022-06-18T16:47:16.113519Z","iopub.status.idle":"2022-06-18T16:47:16.465442Z","shell.execute_reply.started":"2022-06-18T16:47:16.113468Z","shell.execute_reply":"2022-06-18T16:47:16.464618Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:47:17.735056Z","iopub.execute_input":"2022-06-18T16:47:17.735436Z","iopub.status.idle":"2022-06-18T16:47:17.748387Z","shell.execute_reply.started":"2022-06-18T16:47:17.735383Z","shell.execute_reply":"2022-06-18T16:47:17.747133Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def run_style_transfer(content_path,style_path,epochs=500,content_weight=1e3, style_weight=1e-2):\n    \n    model=get_model()\n    \n    for layer in model.layers:\n        layer.trainable = False\n        \n    content_feature,style_feature = get_features(model,content_path,style_path)\n    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n    \n    noise = img_preprocess(content_path)\n    noise=tf.Variable(noise,dtype=tf.float32)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n    \n    best_loss,best_img=float('inf'),None\n    \n    loss_weights = (style_weight, content_weight)\n    dictionary={'model':model,\n              'loss_weights':loss_weights,\n              'image':noise,\n              'gram_style_features':style_gram_matrix,\n              'content_features':content_feature}\n    \n    norm_means = np.array([103.939, 116.779, 123.68])\n    min_vals = -norm_means\n    max_vals = 255 - norm_means   \n  \n    imgs = []\n    for i in range(epochs):\n        grad,all_loss=compute_grads(dictionary)\n        total_loss,style_loss,content_loss=all_loss\n        optimizer.apply_gradients([(grad,noise)])\n        clipped=tf.clip_by_value(noise,min_vals,max_vals)\n        noise.assign(clipped)\n        \n        if total_loss<best_loss:\n            best_loss = total_loss\n            best_img = deprocess_img(noise.numpy())\n            \n         #for visualization   \n            \n        if i%5==0:\n            plot_img = noise.numpy()\n            plot_img = deprocess_img(plot_img)\n            imgs.append(plot_img)\n            IPython.display.clear_output(wait=True)\n            IPython.display.display_png(Image.fromarray(plot_img))\n            print('Epoch: {}'.format(i))        \n            print('Total loss: {:.4e}, ' \n              'style loss: {:.4e}, '\n              'content loss: {:.4e}, '.format(total_loss, style_loss, content_loss))\n    \n    IPython.display.clear_output(wait=True)\n    \n    \n    return best_img,best_loss,imgs","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:47:19.273386Z","iopub.execute_input":"2022-06-18T16:47:19.273745Z","iopub.status.idle":"2022-06-18T16:47:19.289619Z","shell.execute_reply.started":"2022-06-18T16:47:19.273693Z","shell.execute_reply":"2022-06-18T16:47:19.288527Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# **Style Transfer Visualization**","metadata":{}},{"cell_type":"code","source":"best, best_loss,image = run_style_transfer(content_path, \n                                     style_path, epochs=500)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-18T16:47:20.329061Z","iopub.execute_input":"2022-06-18T16:47:20.329451Z","iopub.status.idle":"2022-06-18T16:48:09.725367Z","shell.execute_reply.started":"2022-06-18T16:47:20.329396Z","shell.execute_reply":"2022-06-18T16:48:09.724506Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.title('Style transfer Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:48:09.727293Z","iopub.execute_input":"2022-06-18T16:48:09.727598Z","iopub.status.idle":"2022-06-18T16:48:10.189247Z","shell.execute_reply.started":"2022-06-18T16:48:09.727545Z","shell.execute_reply":"2022-06-18T16:48:10.188386Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.title('Style transfer Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:48:10.190475Z","iopub.execute_input":"2022-06-18T16:48:10.190917Z","iopub.status.idle":"2022-06-18T16:48:10.729668Z","shell.execute_reply.started":"2022-06-18T16:48:10.190871Z","shell.execute_reply":"2022-06-18T16:48:10.727887Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.title('Style transfer Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:48:10.733810Z","iopub.execute_input":"2022-06-18T16:48:10.735157Z","iopub.status.idle":"2022-06-18T16:48:11.346852Z","shell.execute_reply.started":"2022-06-18T16:48:10.735063Z","shell.execute_reply":"2022-06-18T16:48:11.346055Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,15))\nplt.subplot(1,3,3)\nplt.imshow(best)\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,1)\nshow_im(content,'Content Image')\nplt.xticks([])\nplt.yticks([])\nplt.subplot(1,3,2)\nshow_im(style,'Style Image')\nplt.xticks([])\nplt.yticks([])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T16:48:11.349023Z","iopub.execute_input":"2022-06-18T16:48:11.349558Z","iopub.status.idle":"2022-06-18T16:48:11.805675Z","shell.execute_reply.started":"2022-06-18T16:48:11.349509Z","shell.execute_reply":"2022-06-18T16:48:11.804802Z"},"trusted":true},"execution_count":29,"outputs":[]}]}